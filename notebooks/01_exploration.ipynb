{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Crash Prediction - Data Exploration\n",
    "\n",
    "This notebook explores the cloud workload dataset for server crash prediction using machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the Google Cluster Data is downloaded and placed in the 'data/' directory.\n",
    "# You might need to adjust the file path and name based on the actual downloaded data.\n",
    "try:\n",
    "    file_path = '../data/cloud_workload_dataset.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Successfully loaded data from {file_path}\")\n",
    "    print(df.head())\n",
    "    print(df.info())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: {file_path} not found. Please ensure the dataset is downloaded and placed in the 'data/' directory.\")\n",
    "    print(\"You can download the 'Microservices Bottleneck Detection Dataset' from: https://www.kaggle.com/datasets/gagansomashekar/microservices-bottleneck-detection-dataset\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Exploration and Feature Engineering\n\nIn this section we will:\n- Check for missing values\n- Explore the dataset structure and columns\n- Create a binary target variable from `Error_Rate (%)` (high error rate = 1, low = 0)\n- Identify numerical and categorical features\n- One-hot encode categorical variables (Data_Source, Job_Priority, Scheduler_Type, Resource_Allocation_Type)\n- Prepare features (X) and target (y) for model training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for missing values\nprint(\"Missing values:\")\nprint(df.isnull().sum())\nprint(\"\\nDataset shape:\", df.shape)\nprint(\"\\nColumn names:\", df.columns.tolist())\n\n# Create binary target variable from Error_Rate (%)\n# High error rate (above 75th percentile) indicates potential server issues\nerror_threshold = df['Error_Rate (%)'].quantile(0.75)\ndf['high_error'] = (df['Error_Rate (%)'] >= error_threshold).astype(int)\n\nprint(f\"\\nError rate threshold (75th percentile): {error_threshold:.2f}%\")\nprint(f\"Target distribution:\\n{df['high_error'].value_counts()}\")\n\n# Define features and target\n# Exclude: Job_ID (identifier), timestamps, Error_Rate (%) (used to create target), high_error (target)\nexclude_cols = ['Job_ID', 'Task_Start_Time', 'Task_End_Time', 'Error_Rate (%)', 'high_error']\nfeatures = [col for col in df.columns if col not in exclude_cols]\n\ntarget = 'high_error'\n\nprint(f\"\\nFeatures to use: {features}\")\nprint(f\"Target variable: {target}\")\n\n# Separate numerical and categorical features\ncategorical_features = ['Data_Source', 'Job_Priority', 'Scheduler_Type', 'Resource_Allocation_Type']\nnumerical_features = [col for col in features if col not in categorical_features]\n\nprint(f\"\\nNumerical features: {numerical_features}\")\nprint(f\"Categorical features: {categorical_features}\")\n\n# One-hot encode categorical variables\ndf_encoded = pd.get_dummies(df, columns=categorical_features, drop_first=True)\n\n# Update feature list after encoding\nfeatures_encoded = [col for col in df_encoded.columns if col not in exclude_cols and col != target]\n\nX = df_encoded[features_encoded]\ny = df_encoded[target]\n\nprint(f\"\\nFinal feature count: {len(features_encoded)}\")\nprint(f\"Final dataset shape: X={X.shape}, y={y.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Initialize StandardScaler\n# Note: StandardScaler is applied to all features (numerical + one-hot encoded categorical)\n# This is fine for one-hot encoded features as they're already 0/1\nscaler = StandardScaler()\n\n# Fit on training data and transform both training and testing data\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"\\nData standardized using StandardScaler.\")\nprint(f\"Shape of X_train_scaled: {X_train_scaled.shape}\")\nprint(f\"Shape of X_test_scaled: {X_test_scaled.shape}\")\nprint(f\"Target distribution in training set:\\n{y_train.value_counts()}\")\nprint(f\"\\nTarget distribution in test set:\\n{y_test.value_counts()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementation (RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInitializing RandomForestClassifier...\")\n",
    "# You can tune hyperparameters like n_estimators, max_depth, etc.\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"RandomForestClassifier trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating the model...\")\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize Confusion Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Predicted Normal', 'Predicted High Error'],\n            yticklabels=['Actual Normal', 'Actual High Error'])\nplt.title('Confusion Matrix - Server Error Prediction')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()"
  },
  {
   "cell_type": "code",
   "source": "# Feature Importance Analysis\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 10 Most Important Features:\")\nprint(feature_importance.head(10))\n\n# Visualize feature importance\nplt.figure(figsize=(10, 6))\ntop_features = feature_importance.head(15)\nplt.barh(range(len(top_features)), top_features['importance'])\nplt.yticks(range(len(top_features)), top_features['feature'])\nplt.xlabel('Feature Importance')\nplt.title('Top 15 Most Important Features for Server Error Prediction')\nplt.gca().invert_yaxis()\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Steps\n",
    "\n",
    "- Hyperparameter tuning for RandomForestClassifier (e.g., using GridSearchCV or RandomizedSearchCV)\n",
    "- Feature importance analysis: `model.feature_importances_`\n",
    "- Cross-validation\n",
    "- Experiment with other tree-based models (e.g., GradientBoostingClassifier, XGBoost, LightGBM)\n",
    "- Deploy the trained model (e.g., save using joblib or pickle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}