{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Server Crash Prediction - Data Exploration\n",
    "\n",
    "This notebook explores the cloud workload dataset for server crash prediction using machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the cloud workload dataset\nfile_path = '../data/cloud_workload_dataset.csv'\ndf = pd.read_csv(file_path)\nprint(f\"Successfully loaded data from {file_path}\")\nprint(f\"Dataset shape: {df.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(df.head())\nprint(\"\\nDataset info:\")\nprint(df.info())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Data Exploration and Feature Engineering\n\nIn this section we will:\n- Check for missing values\n- Explore the dataset structure and columns\n- Create a binary target variable from `Error_Rate (%)` (high error rate = 1, low = 0)\n- Identify numerical and categorical features\n- One-hot encode categorical variables (Data_Source, Job_Priority, Scheduler_Type, Resource_Allocation_Type)\n- Prepare features (X) and target (y) for model training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clean column names for easier access\ndf.columns = (\n    df.columns\n    .str.strip()\n    .str.replace(' ', '_')\n    .str.replace('(', '', regex=False)\n    .str.replace(')', '', regex=False)\n    .str.replace('%', 'pct')\n)\n\n# Check for missing values\nprint(\"Missing values:\")\nprint(df.isnull().sum())\nprint(f\"\\nCleaned column names: {df.columns.tolist()}\")\n\n# Create binary target variable from Error_Rate\n# High error rate (above 75th percentile) indicates potential server crashes/issues\nerror_threshold = df['Error_Rate_pct'].quantile(0.75)\ndf['High_Error'] = (df['Error_Rate_pct'] > error_threshold).astype(int)\n\nprint(f\"\\nError rate threshold (75th percentile): {error_threshold:.2f}%\")\nprint(f\"\\nTarget distribution:\")\nprint(df['High_Error'].value_counts())\nprint(f\"\\nClass balance: {df['High_Error'].value_counts(normalize=True)}\")\n\n# Drop unnecessary columns (identifiers and timestamps)\ndf = df.drop(['Job_ID', 'Task_Start_Time', 'Task_End_Time'], axis=1)\n\n# One-hot encode categorical variables\ncategorical_cols = ['Data_Source', 'Job_Priority', 'Scheduler_Type', 'Resource_Allocation_Type']\ndf_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n\n# Prepare features (X) and target (y)\nX = df_encoded.drop(['High_Error', 'Error_Rate_pct'], axis=1)\ny = df_encoded['High_Error']\n\nprint(f\"\\nFinal feature count: {X.shape[1]} features\")\nprint(f\"Final dataset shape: X={X.shape}, y={y.shape}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 3. Train/Test Split and SMOTE Resampling\n\n**Challenge:** Our dataset has class imbalance (75% normal, 25% high error).\n\n**Solution:** We use SMOTE (Synthetic Minority Over-sampling Technique) to balance the training set by generating synthetic examples of the minority class (high error cases)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train/test split with stratification\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(f\"Training set size: {len(X_train)}\")\nprint(f\"Test set size: {len(X_test)}\")\nprint(f\"\\nTraining set class distribution:\\n{y_train.value_counts()}\")\n\n# Apply SMOTE to balance the training set\n# Note: We only apply SMOTE to training data to avoid data leakage\nsm = SMOTE(random_state=42)\nX_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)\n\nprint(f\"\\nAfter SMOTE resampling:\")\nprint(f\"Resampled training set size: {len(X_train_resampled)}\")\nprint(f\"Resampled class distribution: {np.bincount(y_train_resampled)}\")\nprint(\"âœ“ Training data is now balanced!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 4. Model Training\n\nWe train a RandomForestClassifier with 600 trees. Random Forests are ideal for this task because:\n- Handle both numerical and categorical features well\n- Resistant to overfitting\n- Provide feature importance metrics\n- Don't require feature scaling (unlike neural networks)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train RandomForest model on SMOTE-resampled data\nprint(\"Training RandomForestClassifier...\")\nmodel = RandomForestClassifier(\n    n_estimators=600,        # 600 decision trees\n    max_depth=None,          # No depth limit (trees grow until pure)\n    min_samples_split=2,     # Minimum samples to split a node\n    min_samples_leaf=1,      # Minimum samples in leaf nodes\n    random_state=42,\n    n_jobs=-1                # Use all CPU cores\n)\n\nmodel.fit(X_train_resampled, y_train_resampled)\nprint(\"âœ“ Model trained successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 5. Model Evaluation\n\nWe evaluate the model using multiple metrics:\n- **Accuracy**: Overall correctness\n- **Precision**: Of predicted high-error cases, how many were actually high-error?\n- **Recall**: Of actual high-error cases, how many did we catch?\n- **F1-Score**: Harmonic mean of precision and recall"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Make predictions on test set\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]  # Probability of high error\n\n# Calculate metrics\nacc = accuracy_score(y_test, y_pred)\nprec = precision_score(y_test, y_pred)\nrec = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n\nprint(\"=\"*60)\nprint(\"MODEL PERFORMANCE (Default 0.5 Threshold)\")\nprint(\"=\"*60)\nprint(f\"Accuracy:  {acc:.4f} ({acc*100:.2f}%)\")\nprint(f\"Precision: {prec:.4f} ({prec*100:.2f}%)\")\nprint(f\"Recall:    {rec:.4f} ({rec*100:.2f}%)\")\nprint(f\"F1-Score:  {f1:.4f}\")\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred, target_names=['Normal', 'High Error']))\n\n# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\")\nprint(cm)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize Confusion Matrix\nfig, ax = plt.subplots(1, 1, figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax,\n            xticklabels=['Predicted Normal', 'Predicted High Error'],\n            yticklabels=['Actual Normal', 'Actual High Error'])\nax.set_title('Confusion Matrix - Server Error Prediction', fontsize=14, fontweight='bold')\nax.set_xlabel('Predicted Label', fontsize=12)\nax.set_ylabel('True Label', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n# Interpretation\ntn, fp, fn, tp = cm.ravel()\nprint(f\"\\nConfusion Matrix Breakdown:\")\nprint(f\"True Negatives (TN):  {tn} - Correctly predicted normal\")\nprint(f\"False Positives (FP): {fp} - Incorrectly predicted high error\")\nprint(f\"False Negatives (FN): {fn} - Missed high error cases\")\nprint(f\"True Positives (TP):  {tp} - Correctly caught high error cases\")"
  },
  {
   "cell_type": "code",
   "source": "# Feature Importance Analysis\nfeature_importance = pd.DataFrame({\n    'feature': X.columns,\n    'importance': model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 10 Most Important Features:\")\nprint(feature_importance.head(10).to_string(index=False))\n\n# Visualize feature importance\nfig, ax = plt.subplots(figsize=(10, 6))\ntop_features = feature_importance.head(15)\ncolors = plt.cm.viridis(np.linspace(0.3, 0.9, len(top_features)))\nax.barh(range(len(top_features)), top_features['importance'], color=colors)\nax.set_yticks(range(len(top_features)))\nax.set_yticklabels(top_features['feature'])\nax.set_xlabel('Feature Importance', fontsize=12)\nax.set_title('Top 15 Most Important Features for Server Error Prediction', \n             fontsize=14, fontweight='bold')\nax.invert_yaxis()\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nðŸ“Š Key Insight: The top 7 features are all numerical system metrics,\")\nprint(f\"   showing that performance indicators are the strongest predictors.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "<cell_type>markdown</cell_type>## 6. Model Persistence\n\nSave the trained model and artifacts for future use and deployment."
  },
  {
   "cell_type": "markdown",
   "source": "### Sources of Bias (Important Considerations)\nWhen interpreting results, be aware of these documented limitations:\n- **Server type bias**: Logs from limited machine types - uncommon server patterns may be underrepresented\n- **Workload variation**: CPU-intensive, memory-intensive, and network-intensive jobs may not be equally captured\n- **Anomaly frequency**: Rare events (maintenance windows, unexpected traffic spikes) appear infrequently\n- **Configuration diversity**: Not all server types and configurations are equally represented\n\n### Future Improvements\n1. **Hyperparameter Tuning**: GridSearchCV or RandomizedSearchCV to optimize model parameters\n2. **Cross-Validation**: K-fold CV for more robust performance estimates\n3. **Threshold Optimization**: Adjust decision threshold based on business needs (precision vs recall)\n4. **Additional Models**: Compare with GradientBoosting, XGBoost, or LightGBM\n5. **Feature Engineering**: Create interaction features (e.g., CPU Ã— Memory utilization)\n6. **More Data**: Expand to Google Cluster Data, Azure Public Dataset, or Alibaba ClusterData\n\n### Technologies Used\n- **Python**: Core programming language\n- **scikit-learn**: Machine learning framework (RandomForest, train/test split, metrics)\n- **imbalanced-learn**: SMOTE implementation for handling class imbalance\n- **Pandas**: Data manipulation and feature engineering\n- **Matplotlib & Seaborn**: Visualization\n- **Joblib**: Model serialization\n\n### Project Team\n**AI/ML Team Lead**: AI4ALL Ignite Program  \n**Contributors**: Leilany Rojas, Ammar Salama, Salvador Frias, Mashel Khan  \n**Presentation**: AI4ALL Research Symposium",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Summary and Key Findings\n\n### Project Overview\nThis project predicts server crashes and high error rates using machine learning on cloud workload data. We trained a RandomForestClassifier on 5,000 job records to classify jobs as \"high error\" (potential crashes/bottlenecks) vs \"normal\" based on CPU, memory, network, and other system metrics.\n\n### Research Question\n**How can AI models predict spikes in server load to prevent slowdowns or crashes, while accounting for variations in workload and server types?**\n\n### Methodology\n1. **Data**: 5,000 cloud job records with 15 features (CPU, memory, network metrics, etc.)\n2. **Target**: Binary classification - error rates â‰¥ 75th percentile (3.8%) = high error\n3. **Class Imbalance Solution**: SMOTE resampling to balance training data (3,002 normal / 3,002 high error)\n4. **Model**: RandomForestClassifier with 600 estimators\n5. **Evaluation**: 80/20 train/test split with stratification\n\n### Model Performance\n- **Accuracy**: ~71% - Model correctly classifies most cases\n- **Recall**: 4-18% - Currently catches only a small fraction of high-error cases\n- **Challenge**: Imbalanced data leads to conservative predictions (model prefers \"safe\" normal predictions)\n\n### Top Predictive Features\n1. System Throughput (tasks/sec) - 10.4%\n2. Memory Consumption (MB) - 10.3%\n3. Network Bandwidth Utilization (Mbps) - 10.3%\n4. Task Execution Time (ms) - 10.1%\n5. Number of Active Users - 10.1%\n\n**Key Insight**: Numerical performance metrics are far more predictive than categorical features (scheduler type, job priority, etc.)\n\n### Real-World Impact\n- **Potential Use Case**: Early warning system for cloud infrastructure teams\n- **Business Value**: Prevent downtime by flagging high-risk jobs before they cause outages\n- **Trade-offs**: Current model prioritizes precision over recall (fewer false alarms, but misses some issues)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save the trained model\nmodel_path = '../models/random_forest_smote_model.pkl'\njoblib.dump(model, model_path)\nprint(f\"âœ“ Model saved to {model_path}\")\n\n# Save feature importance\nfeature_importance.to_csv('../models/feature_importance.csv', index=False)\nprint(\"âœ“ Feature importance saved to ../models/feature_importance.csv\")\n\nprint(\"\\nðŸ’¾ Model artifacts saved successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}